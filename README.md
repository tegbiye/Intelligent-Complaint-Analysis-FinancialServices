# Intelligent Complaint Analysis for Financial Services

## Building a RAG-Powered Chatbot to Turn Customer Feedback into Actionable Insights

## Task-1: Exploratory Data Analysis and Data Preprocessing

#### EDA and Preprocessing Findings

âœ… The exploratory data analysis of the CFPB complaint dataset reveals key insights into its structure and content.The dataset contains a diverse set of complaints across multiple financial products, with the initial analysis showing the distribution of complaints across products, highlighting which financial products (e.g., Credit card, Personal loan) receive the most complaints.

âœ… The narrative length analysis indicates a wide range of complaint lengths, with some narratives being very short (<10 words) and others excessively long (>500 words), suggesting variability in consumer detail.

âœ… Complaints without narratives were identified, and these were excluded from the final dataset to ensure quality for the RAG pipeline.

âœ… After filtering for the specified products (Credit card, Personal loan, Buy Now, Pay Later, Savings account, Money transfers) and removing records with empty narratives, the dataset was significantly reduced in size, ensuring relevance and usability.

âœ… Text cleaning involved lowercasing, removing special characters, and eliminating common boilerplate phrases to improve embedding quality.

âœ… The cleaned dataset, saved as **filtered_complaints.csv**, retains essential metadata and cleaned narratives, making it suitable for downstream tasks like embedding and retrieval.

âœ… The preprocessing steps ensure that the narratives are standardized and free of noise, enhancing the performance of the chatbot in answering queries based on real-world feedback.

## Task-2 Text Chunking, Embedding, and Vector Store Indexing

#### Report Section: Chunking Strategy and Embedding Model Choice

âœ… For the text chunking strategy, I utilized LangChain's RecursiveCharacterTextSplitter with a chunk_size of 500 characters and a chunk_overlap of 50 characters.

âœ… The chunk size was chosen to balance capturing coherent segments of complaint narratives (approximately 100-150 words) while ensuring embeddings remain semantically meaningful. Complaints often contain distinct issues (e.g., billing disputes, customer service issues), and smaller chunks help isolate these for better retrieval precision.

âœ… The overlap of 50 characters maintains context across chunk boundaries, especially for narratives split mid-sentence. I experimented with larger chunk sizes (e.g., 1000 characters), but they risked diluting specific issues in longer narratives, while smaller chunks (e.g., 200 characters) fragmented context excessively. The chosen parameters were validated by inspecting sample chunks, ensuring they retained meaningful complaint details.

âœ… The sentence-transformers/all-MiniLM-L6-v2 model was selected for embedding due to its efficiency and performance in semantic similarity tasks. This lightweight model (22M parameters, 384-dimensional embeddings) is optimized for short text, making it ideal for complaint narratives, which are typically concise yet descriptive. It provides a good balance between embedding quality and computational efficiency, suitable for indexing large datasets like the CFPB complaints. The modelâ€™s pre-training on diverse datasets ensures robust handling of financial terminology and consumer language.

âœ… FAISS was chosen for the vector store due to its speed and scalability for similarity search, with metadata (complaint ID, product, chunk ID) stored alongside each embedding to enable traceability to the original complaint. The vector store and metadata are persisted in the vector_store/ directory for downstream retrieval tasks.

## Task 3: Building the RAG Core Logic and Evaluation

#### Deliverables

    1. Python Module (rag_pipeline.py): 
     - The script successfully produced the evaluation table
    2. Evaluation Table (evaluation_table.md)
     - The table (provided in the document) contains answers and sources for five questions, with a quality score  
       and a comment to â€œReview answer and sources for accuracy and relevance.â€
## Project Structure

<pre>
Intelligent-Complaint-Analysis-Financial Services/
â”œâ”€â”€ .github/workflows/ci.yml   # For CI/CD
â”œâ”€â”€ data/                       # add this folder to .gitignore
|   â”œâ”€â”€ evaluation_table.md     # evaluation table generated 
â”‚   â”œâ”€â”€ raw/                   # Raw data goes here 
â”‚   â””â”€â”€ processed/             # Processed data for training
â”œâ”€â”€ vectore_store/
|   â”œâ”€â”€ sample_chunks.csv    # Sample chunks for verification
|   â”œâ”€â”€ metadata.pkl         # chunks metadata
|   â”œâ”€â”€ faiss_index.faiss         #  FAISS index
|   â””â”€â”€ faiss_index.bin      # FAISS index
â”œâ”€â”€ models/                  # Saved embedding model
â”œâ”€â”€ notebooks/
|   â”œâ”€â”€ README.md
|   â”œâ”€â”€ RAG-pipeline.ipynb           # Pipeline notebook
|   â”œâ”€â”€ text_chunk.ipynb             # Text chunk, embedding notebook
â”‚   â””â”€â”€ complaints-EDA.ipynb          # Exploratory, one-off analysis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
|   â”œâ”€â”€ rag_helper.py     # helper script gopt the rag
â”‚   â”œâ”€â”€ data_process.py     # Script for Data Processing (EDA)
|   â”œâ”€â”€ text_chunker.py     # Helper function for the text chunking
â”‚   â””â”€â”€ loggers.py    # logging to the files and output
â”œâ”€â”€ tests/
|   â”œâ”€â”€ __init__.py
|   â”œâ”€â”€ test_chunk_narratives.py
|   â”œâ”€â”€ test_retrieval_rag.py
â”‚   â””â”€â”€ test_sample.py         # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
</pre>

## Getting Started

Clone the repository

`git clone http://github.com/tegbiye/Intelligent-Complaint-Analysis-FinancialServices.git`

`cd Intelligent-Complaint-Analysis-FinancialServices`

Create environment using venv

`python -m venv .venv`

Activate the environment

`.venv\Scripts\activate` (Windows)

`source .venv\bin\activate` (Linux / Mac)

Install Dependencies

`pip install -r requirements.txt`

ğŸ“œ License This project is licensed under the MIT License. Feel free to use, modify, and distribute with proper attribution.
