{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3876df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6888a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623e0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fb57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from faiss import IndexFlatL2\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd76525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import data_loader\n",
    "from text_chunker import chunk_narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9b8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-19 21:01:19,099 - INFO - Data loaded successfully from ../data/processed/filtered_complaints.csv\n"
     ]
    }
   ],
   "source": [
    "df = data_loader('../data/processed/filtered_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c0def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # ~100-150 words, suitable for coherent complaint segments\n",
    "    chunk_overlap=50,  # Small overlap to maintain context across chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8016651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 15055\n"
     ]
    }
   ],
   "source": [
    "# Generate chunks\n",
    "chunks, metadata = chunk_narratives(df, text_splitter)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753b7b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fcc5198c5c4a83ba07b239bb5048d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (15055, 384)\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate Embeddings\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(chunks, batch_size=32, show_progress_bar=True)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b253f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save('../models/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28935a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create and Populate FAISS Index\n",
    "dimension = embeddings.shape[1]\n",
    "index = IndexFlatL2(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d6eb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save Vector Store and Metadata\n",
    "vector_store_path = '../vector_store/faiss_index.bin'\n",
    "metadata_path = '../vector_store/metadata.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1264259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "with open(vector_store_path, 'wb') as f:\n",
    "    pickle.dump(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d831d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump({'chunks': chunks, 'metadata': metadata}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45207ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to: ../vector_store/faiss_index.bin\n",
      "Metadata saved to: ../vector_store/metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector store saved to: {vector_store_path}\")\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c916eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4541 entries, 0 to 4540\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   Consumer complaint narrative  4541 non-null   object\n",
      " 1   Product                       4541 non-null   object\n",
      " 2   Issue                         4541 non-null   object\n",
      " 3   Company                       4541 non-null   object\n",
      " 4   Date received                 4541 non-null   object\n",
      " 5   narrative_length              4541 non-null   int64 \n",
      " 6   cleaned_narrative             4541 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 248.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9fb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks saved to: ../vector_store/sample_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "# Save a sample of chunks for verification\n",
    "sample_chunks = pd.DataFrame({\n",
    "    'chunk_id': [m['chunk_id'] for m in metadata],\n",
    "    'product': [m['product'] for m in metadata],\n",
    "    'chunk_text': chunks\n",
    "})\n",
    "sample_chunks.head(10).to_csv('../vector_store/sample_chunks.csv', index=False)\n",
    "print(f\"Sample chunks saved to: {'../vector_store/sample_chunks.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2938e7",
   "metadata": {},
   "source": [
    "### Report Section: Chunking Strategy and Embedding Model Choice\n",
    "\n",
    "For the text chunking strategy, I utilized LangChain's RecursiveCharacterTextSplitter with a chunk_size of 500 characters and a chunk_overlap of 50 characters. The chunk size was chosen to balance capturing coherent segments of complaint narratives (approximately 100-150 words) while ensuring embeddings remain semantically meaningful. Complaints often contain distinct issues (e.g., billing disputes, customer service issues), and smaller chunks help isolate these for better retrieval precision. \n",
    "The overlap of 50 characters maintains context across chunk boundaries, especially for narratives split mid-sentence. I experimented with larger chunk sizes (e.g., 1000 characters), but they risked diluting specific issues in longer narratives, while smaller chunks (e.g., 200 characters) fragmented context excessively. The chosen parameters were validated by inspecting sample chunks, ensuring they retained meaningful complaint details.\n",
    "\n",
    "The sentence-transformers/all-MiniLM-L6-v2 model was selected for embedding due to its efficiency and performance in semantic similarity tasks. This lightweight model (22M parameters, 384-dimensional embeddings) is optimized for short text, making it ideal for complaint narratives, which are typically concise yet descriptive. It provides a good balance between embedding quality and computational efficiency, suitable for indexing large datasets like the CFPB complaints. The modelâ€™s pre-training on diverse datasets ensures robust handling of financial terminology and consumer language. \n",
    "FAISS was chosen for the vector store due to its speed and scalability for similarity search, with metadata (complaint ID, product, chunk ID) stored alongside each embedding to enable traceability to the original complaint. The vector store and metadata are persisted in the vector_store/ directory for downstream retrieval tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
